{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    # Load CSV containing image paths and labels\n",
    "    csv_path = \"C:/Users/sk731/OneDrive/Desktop/MLDSA2/train.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Define image and mask directories\n",
    "    img_dir = \"C:/Users/sk731/OneDrive/Desktop/MLDSA2/train/my_train_img\"\n",
    "    mask_dir = \"C:/Users/sk731/OneDrive/Desktop/MLDSA2/train/my_train_seg\"\n",
    "    \n",
    "    # Read image and mask paths\n",
    "    img_paths = [os.path.join(img_dir, fname) for fname in df['id']]\n",
    "    mask_paths = [os.path.join(mask_dir, fname.replace(\".png\", \".png\")) for fname in df['id']]\n",
    "    \n",
    "    # Load images and masks\n",
    "    images = [tf.image.decode_png(tf.io.read_file(fname)) for fname in img_paths]\n",
    "    masks = [tf.image.decode_png(tf.io.read_file(fname)) for fname in mask_paths]\n",
    "    \n",
    "    # Normalize images and masks\n",
    "    images = [tf.cast(img, tf.float32) / 255.0 for img in images]\n",
    "    masks = [tf.cast(mask, tf.float32) / 255.0 for mask in masks]\n",
    "    \n",
    "    # Convert masks to binary format\n",
    "    masks = [tf.where(mask > 0.5, 1.0, 0.0) for mask in masks]\n",
    "    \n",
    "    return images, masks\n",
    "\n",
    "# Define Dice coefficient metric\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    return (2.0 * intersection) / (union + 1e-7)  # Adding a small epsilon to avoid division by zero\n",
    "\n",
    "# Define UNet++ model architecture\n",
    "def unet_plus_plus(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    \n",
    "    # Decoder\n",
    "    up1 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv3)\n",
    "    up1 = Concatenate()([conv2, up1])\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    up2 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv4)\n",
    "    up2 = Concatenate()([conv1, up2])\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Output layer\n",
    "     # Output layer\n",
    "    outputs = Conv2D(3, 1, activation='sigmoid')(conv5)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Load data\n",
    "images, masks = load_data()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model parameters\n",
    "input_shape = images[0].shape\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "\n",
    "# Define UNet++ model\n",
    "model = unet_plus_plus(input_shape)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coefficient])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"unet_plus_plus_model.h5\", monitor='val_dice_coefficient', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_dice_coefficient', patience=5, verbose=1, mode='max', restore_best_weights=True)\n",
    "# Convert lists of images to a single tensor\n",
    "X_train = tf.stack(X_train)\n",
    "X_test = tf.stack(X_test)\n",
    "\n",
    "# Convert lists of masks to a single tensor\n",
    "y_train = tf.stack(y_train)\n",
    "y_test = tf.stack(y_test)\n",
    "\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=0.1, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate model on test data\n",
    "loss, dice_coefficient = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Dice Coefficient:\", dice_coefficient)\n",
    "\n",
    "    \n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
